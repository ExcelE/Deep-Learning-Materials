{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing with Tensorboard\n",
    "\n",
    "The most popular use of Tensorboard is to manage multiple models. When training there are a lot of fine tuning to do that it gets tedious. Some parameters like the structure of the neural network, the optimizer used, the learning rate in the optimizer, etc. permutates to thousands to millions of possible models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The things we are going to tweak in:\n",
    "\n",
    "```model.add(Conv2D(64, (3,3), input_shape(X.shape[1:]))```\n",
    "\n",
    "are the kernel size ```(3,3)```, number of layers, number of nodes per layer ```64```, and the choice of having a dense layer at the end."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dense_layers = [0, 1, 2]\n",
    "layer_sizes = [32, 64, 128]\n",
    "conv_layers = [1, 2, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are permutating to all the possibilities based on the options above."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for dense_layer in dense_layers:\n",
    "    for layer_size in layer_sizes:\n",
    "        for conv_layer in conv_layers:\n",
    "            NAME = \"{}-conv-{}-nodes-{}-dense-{}\".format(conv_layer, layer_size, dense_layer, int(time.time()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see it come together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(\"X_rgb.pickle\",\"rb\")\n",
    "X = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"y_rgb.pickle\",\"rb\")\n",
    "y = pickle.load(pickle_in)\n",
    "\n",
    "X = X/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layers = [0, 1, 2]\n",
    "layer_sizes = [32, 64, 128]\n",
    "conv_layers = [1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17462 samples, validate on 7484 samples\n",
      "Epoch 1/10\n",
      "17462/17462 [==============================] - 16s 943us/step - loss: 0.6261 - acc: 0.6643 - val_loss: 0.5547 - val_acc: 0.7217\n",
      "Epoch 2/10\n",
      "17462/17462 [==============================] - 13s 765us/step - loss: 0.5117 - acc: 0.7517 - val_loss: 0.5267 - val_acc: 0.7432\n",
      "Epoch 3/10\n",
      "17462/17462 [==============================] - 13s 771us/step - loss: 0.4594 - acc: 0.7843 - val_loss: 0.5264 - val_acc: 0.7410\n",
      "Epoch 4/10\n",
      "17462/17462 [==============================] - 13s 769us/step - loss: 0.4094 - acc: 0.8137 - val_loss: 0.5504 - val_acc: 0.7342\n",
      "Epoch 5/10\n",
      "17462/17462 [==============================] - 13s 767us/step - loss: 0.3605 - acc: 0.8426 - val_loss: 0.5587 - val_acc: 0.7338\n",
      "Epoch 6/10\n",
      "17462/17462 [==============================] - 13s 771us/step - loss: 0.3123 - acc: 0.8677 - val_loss: 0.5713 - val_acc: 0.7423\n",
      "Epoch 7/10\n",
      "17462/17462 [==============================] - 14s 777us/step - loss: 0.2684 - acc: 0.8900 - val_loss: 0.5945 - val_acc: 0.7380\n",
      "Epoch 8/10\n",
      "17462/17462 [==============================] - 14s 799us/step - loss: 0.2356 - acc: 0.9092 - val_loss: 0.6443 - val_acc: 0.7251\n",
      "Epoch 9/10\n",
      "17462/17462 [==============================] - 16s 904us/step - loss: 0.1985 - acc: 0.9290 - val_loss: 0.6769 - val_acc: 0.7354\n",
      "Epoch 10/10\n",
      "17462/17462 [==============================] - 14s 778us/step - loss: 0.1703 - acc: 0.9414 - val_loss: 0.7212 - val_acc: 0.7309\n",
      "Train on 17462 samples, validate on 7484 samples\n",
      "Epoch 1/10\n",
      "17462/17462 [==============================] - 14s 827us/step - loss: 0.5906 - acc: 0.6806 - val_loss: 0.5395 - val_acc: 0.7290\n",
      "Epoch 2/10\n",
      "17462/17462 [==============================] - 14s 826us/step - loss: 0.4995 - acc: 0.7576 - val_loss: 0.4917 - val_acc: 0.7684\n",
      "Epoch 3/10\n",
      "17462/17462 [==============================] - 14s 827us/step - loss: 0.4430 - acc: 0.7956 - val_loss: 0.4764 - val_acc: 0.7798\n",
      "Epoch 4/10\n",
      "17462/17462 [==============================] - 14s 814us/step - loss: 0.4064 - acc: 0.8154 - val_loss: 0.4496 - val_acc: 0.7898\n",
      "Epoch 5/10\n",
      "17462/17462 [==============================] - 14s 815us/step - loss: 0.3629 - acc: 0.8397 - val_loss: 0.4442 - val_acc: 0.7976\n",
      "Epoch 6/10\n",
      "17462/17462 [==============================] - 15s 869us/step - loss: 0.3330 - acc: 0.8562 - val_loss: 0.4523 - val_acc: 0.8034\n",
      "Epoch 7/10\n",
      "17462/17462 [==============================] - 20s 1ms/step - loss: 0.2996 - acc: 0.8725 - val_loss: 0.4843 - val_acc: 0.7958\n",
      "Epoch 8/10\n",
      "17462/17462 [==============================] - 14s 818us/step - loss: 0.2719 - acc: 0.8844 - val_loss: 0.4666 - val_acc: 0.8081\n",
      "Epoch 9/10\n",
      "17462/17462 [==============================] - 14s 809us/step - loss: 0.2456 - acc: 0.8986 - val_loss: 0.4724 - val_acc: 0.8123\n",
      "Epoch 10/10\n",
      "17462/17462 [==============================] - 14s 808us/step - loss: 0.2188 - acc: 0.9131 - val_loss: 0.5417 - val_acc: 0.7859\n",
      "Train on 17462 samples, validate on 7484 samples\n",
      "Epoch 1/10\n",
      "17462/17462 [==============================] - 17s 980us/step - loss: 0.6348 - acc: 0.6298 - val_loss: 0.5559 - val_acc: 0.7181\n",
      "Epoch 2/10\n",
      "17462/17462 [==============================] - 15s 846us/step - loss: 0.5177 - acc: 0.7429 - val_loss: 0.5244 - val_acc: 0.7481\n",
      "Epoch 3/10\n",
      "17462/17462 [==============================] - 37s 2ms/step - loss: 0.4600 - acc: 0.7839 - val_loss: 0.4607 - val_acc: 0.7794\n",
      "Epoch 4/10\n",
      "17462/17462 [==============================] - 19s 1ms/step - loss: 0.4220 - acc: 0.8043 - val_loss: 0.4511 - val_acc: 0.7930\n",
      "Epoch 5/10\n",
      "17462/17462 [==============================] - 15s 838us/step - loss: 0.3896 - acc: 0.8264 - val_loss: 0.4230 - val_acc: 0.8092\n",
      "Epoch 6/10\n",
      "17462/17462 [==============================] - 15s 875us/step - loss: 0.3557 - acc: 0.8449 - val_loss: 0.4223 - val_acc: 0.8075\n",
      "Epoch 7/10\n",
      "17462/17462 [==============================] - 21s 1ms/step - loss: 0.3276 - acc: 0.8584 - val_loss: 0.3932 - val_acc: 0.8328\n",
      "Epoch 8/10\n",
      "17462/17462 [==============================] - 15s 868us/step - loss: 0.3091 - acc: 0.8658 - val_loss: 0.3880 - val_acc: 0.8300\n",
      "Epoch 9/10\n",
      "17462/17462 [==============================] - 15s 834us/step - loss: 0.2915 - acc: 0.8741 - val_loss: 0.4113 - val_acc: 0.8226\n",
      "Epoch 10/10\n",
      "17462/17462 [==============================] - 14s 830us/step - loss: 0.2619 - acc: 0.8887 - val_loss: 0.3769 - val_acc: 0.8402\n",
      "Train on 17462 samples, validate on 7484 samples\n",
      "Epoch 1/10\n",
      "17462/17462 [==============================] - 17s 998us/step - loss: 0.6382 - acc: 0.6477 - val_loss: 0.5523 - val_acc: 0.7201\n",
      "Epoch 2/10\n",
      "17462/17462 [==============================] - 16s 943us/step - loss: 0.4953 - acc: 0.7610 - val_loss: 0.5414 - val_acc: 0.7346\n",
      "Epoch 3/10\n",
      "17462/17462 [==============================] - 17s 953us/step - loss: 0.4292 - acc: 0.8080 - val_loss: 0.5286 - val_acc: 0.7493\n",
      "Epoch 4/10\n",
      "17462/17462 [==============================] - 19s 1ms/step - loss: 0.3766 - acc: 0.8342 - val_loss: 0.5782 - val_acc: 0.7277\n",
      "Epoch 5/10\n",
      "17462/17462 [==============================] - 17s 987us/step - loss: 0.3220 - acc: 0.8627 - val_loss: 0.5983 - val_acc: 0.7305\n",
      "Epoch 6/10\n",
      "17462/17462 [==============================] - 16s 932us/step - loss: 0.2758 - acc: 0.8839 - val_loss: 0.6369 - val_acc: 0.7344\n",
      "Epoch 7/10\n",
      "17462/17462 [==============================] - 16s 931us/step - loss: 0.2265 - acc: 0.9120 - val_loss: 0.6516 - val_acc: 0.7370\n",
      "Epoch 8/10\n",
      "17462/17462 [==============================] - 16s 932us/step - loss: 0.1826 - acc: 0.9340 - val_loss: 0.7235 - val_acc: 0.7365\n",
      "Epoch 9/10\n",
      "17462/17462 [==============================] - 17s 994us/step - loss: 0.1501 - acc: 0.9487 - val_loss: 0.7759 - val_acc: 0.7249\n",
      "Epoch 10/10\n",
      "17462/17462 [==============================] - 18s 1ms/step - loss: 0.1196 - acc: 0.9646 - val_loss: 0.8323 - val_acc: 0.7297\n",
      "Train on 17462 samples, validate on 7484 samples\n",
      "Epoch 1/10\n",
      "17462/17462 [==============================] - 19s 1ms/step - loss: 0.6258 - acc: 0.6415 - val_loss: 0.5698 - val_acc: 0.7106\n",
      "Epoch 2/10\n",
      "17462/17462 [==============================] - 18s 1ms/step - loss: 0.5468 - acc: 0.7240 - val_loss: 0.5367 - val_acc: 0.7392\n",
      "Epoch 3/10\n",
      "17462/17462 [==============================] - 18s 1ms/step - loss: 0.4829 - acc: 0.7679 - val_loss: 0.4919 - val_acc: 0.7650\n",
      "Epoch 4/10\n",
      "17462/17462 [==============================] - 18s 1ms/step - loss: 0.4410 - acc: 0.7965 - val_loss: 0.4884 - val_acc: 0.7636\n",
      "Epoch 5/10\n",
      "17462/17462 [==============================] - 18s 1ms/step - loss: 0.4050 - acc: 0.8145 - val_loss: 0.4615 - val_acc: 0.7912\n",
      "Epoch 6/10\n",
      "17462/17462 [==============================] - 18s 1ms/step - loss: 0.3708 - acc: 0.8354 - val_loss: 0.4675 - val_acc: 0.7839\n",
      "Epoch 7/10\n",
      "17462/17462 [==============================] - 18s 1ms/step - loss: 0.3345 - acc: 0.8530 - val_loss: 0.4885 - val_acc: 0.7875\n",
      "Epoch 8/10\n",
      "17462/17462 [==============================] - 18s 1ms/step - loss: 0.2980 - acc: 0.8719 - val_loss: 0.5018 - val_acc: 0.7902\n",
      "Epoch 9/10\n",
      "17462/17462 [==============================] - 18s 1ms/step - loss: 0.2724 - acc: 0.8833 - val_loss: 0.5886 - val_acc: 0.7700\n",
      "Epoch 10/10\n",
      "12384/17462 [====================>.........] - ETA: 3s - loss: 0.2686 - acc: 0.8848"
     ]
    }
   ],
   "source": [
    "for dense_layer in dense_layers:\n",
    "    for layer_size in layer_sizes:\n",
    "        for conv_layer in conv_layers:\n",
    "            NAME = \"{}-conv-{}-nodes-{}-dense-{}\".format(conv_layer, layer_size, dense_layer, int(time.time()))\n",
    "            tensorboard = TensorBoard(log_dir='logs/{}'.format(NAME))\n",
    "            \n",
    "            model = Sequential()\n",
    "\n",
    "            model.add(Conv2D(layer_size, (3, 3), input_shape=X.shape[1:]))\n",
    "            model.add(Activation('relu'))\n",
    "            model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "            for l in range(conv_layer - 1):\n",
    "                model.add(Conv2D(layer_size, (3, 3)))\n",
    "                model.add(Activation('relu'))\n",
    "                model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "            model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "\n",
    "            for l in range(dense_layer):\n",
    "                model.add(Dense(layer_size))\n",
    "                model.add(Activation('relu'))\n",
    "\n",
    "            model.add(Dense(1))\n",
    "            model.add(Activation('sigmoid'))\n",
    "            \n",
    "            model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "            \n",
    "            model.fit(X, y, batch_size=32, epochs=10, validation_split=0.3, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
